{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c26d477-f1d5-4e68-b45a-e3ebeac755fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"pip install transformers\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a80898a-0c3a-4f21-ba5e-ebbab2a0699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 16:37:50.099070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-07 16:37:50.236146: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-07 16:37:50.909827: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-07 16:37:50.909913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-07 16:37:50.909922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import gc\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "# from pandarallel import pandarallel\n",
    "\n",
    "# pandarallel.initialize(nb_workers=12)\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fa6948-0620-43f1-916e-cc8d71811e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# SM_CHANNEL_TRAIN = \"/opt/ml/input/data/train\"\n",
    "# SM_MODEL_DIR = \"/opt/ml/model\"\n",
    "\n",
    "SM_CHANNEL_TRAIN = \"pretrain_data/\"\n",
    "SM_MODEL_DIR = \"/opt/ml/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9a8b5c-ea4a-4d92-80e5-57842af6abef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    \n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "    \n",
    "    pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True, name=\"shared_embedding\") \n",
    "        self.output_bias = self.add_weight(shape=(vocab_size,), initializer=\"zeros\", trainable=True, name=\"output_bias\")\n",
    "    \n",
    "    def call(self, x, embedding_type):\n",
    "        if embedding_type == \"input\":\n",
    "            x = self.embedding(x)\n",
    "            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "            return x\n",
    "        elif embedding_type == \"output\":\n",
    "            x = tf.matmul(x, self.embedding.embeddings, transpose_b=True)\n",
    "            return x + self.output_bias\n",
    "        else:\n",
    "            raise ValueError(\"embedding_type must be 'input' or 'output'\")\n",
    "        \n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = positional_encoding(length=max_len, depth=d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75785a98-4508-4b73-97eb-b63cf2df7db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# attention\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "        \n",
    "        # Cache the attention scores for plotting later.\n",
    "        # self.last_attn_scores = attn_scores\n",
    "        \n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77131ca8-c406-48a2-90a7-6cc2b9fb0adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feedforward\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87676420-2476-412f-9062-34d307300739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model//num_heads,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58671078-06de-411e-8447-5600ddefb82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decoder\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model//num_heads,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model//num_heads,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, gpt_mode=False):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        \n",
    "        if not gpt_mode: \n",
    "            x = self.cross_attention(x=x, context=context)\n",
    "        \n",
    "        # Cache the last attention scores for plotting later\n",
    "        # self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "        \n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "                 dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "    \n",
    "        # self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context, gpt_mode=False):\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context, gpt_mode)\n",
    "    \n",
    "        # self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "    \n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99535292-ce4d-47e5-9527-6a71ed07603c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(SM_CHANNEL_TRAIN, \"btokenizer\"))\n",
    "\n",
    "new_spl_tokens = [\"[SOS]\", \"[EOS]\", \"[SPANEND]\", \"[MS]\", \"[MT]\", \"[NT]\"] + [f\"[SPAN{i}]\" for i in range(50)]\n",
    "\n",
    "tokenizer.add_tokens(new_tokens=new_spl_tokens, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b15c3f-7e7f-42f8-90ce-0d110886e1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# num_layers = 12\n",
    "# d_model = 768\n",
    "# dff = 3072\n",
    "# num_heads = 12\n",
    "# dropout_rate = 0.1\n",
    "# max_len = 256\n",
    "# corr_prob = 0.15\n",
    "# vocab_size = len(tokenizer)\n",
    "\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "max_len = 256\n",
    "corr_prob = 0.15\n",
    "vocab_size = len(tokenizer)\n",
    "batch_size=512\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd6ccebc-879c-4030-84db-80a8b8487f03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "\n",
    "with open(os.path.join(SM_CHANNEL_TRAIN, \"raw_text.txt\"), \"r\") as f:\n",
    "    data = f.read().split(\"\\n\")\n",
    "\n",
    "data = list(set(data))\n",
    "\n",
    "df = pd.DataFrame({\"text\": data}).sample(frac=1.0, ignore_index=True)\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d166b306-89fd-480b-b95a-6a0472e79fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "class BatchGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size, tokenizer=tokenizer, max_len=max_len, corr_prob=corr_prob):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.corr_prob = corr_prob\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.df) % self.batch_size == 0:\n",
    "            return len(self.df) // self.batch_size\n",
    "        return len(self.df) // self.batch_size + 1\n",
    "\n",
    "    def __create_ms_input_and_output__(self, text):\n",
    "        corr_count = 0\n",
    "        text_tokens = text.split(\" \")\n",
    "        input_tokens = [\"[MS]\"]\n",
    "        output_tokens = []\n",
    "        prev_token_corr = False\n",
    "        for token in text_tokens:\n",
    "            if np.random.random() > self.corr_prob:\n",
    "                input_tokens.append(token)\n",
    "                prev_token_corr = False\n",
    "            else:\n",
    "                if not prev_token_corr:\n",
    "                    input_tokens.append(f\"[SPAN{corr_count}]\")\n",
    "                    \n",
    "                    output_tokens.append(f\"[SPAN{corr_count}]\")\n",
    "                    corr_count += 1\n",
    "                output_tokens.append(token)\n",
    "                prev_token_corr = True\n",
    "                \n",
    "        output_tokens.append(\"[SPANEND]\")\n",
    "\n",
    "        return \" \".join(input_tokens), \" \".join(output_tokens[:-1]), \" \".join(output_tokens[1:])\n",
    "    \n",
    "    def __create_mt_input_and_output__(self, text):\n",
    "        \n",
    "        tokens = self.tokenizer(text, add_special_tokens=False, max_length=self.max_len-1, padding=\"max_length\", \n",
    "                                truncation=True, return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        pad_token_id = self.tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "        mask_token_id = self.tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "        mt_encoder_input_tokens = [self.tokenizer.convert_tokens_to_ids(\"[MT]\")]\n",
    "        mt_encoder_output_tokens = [pad_token_id]\n",
    "        mask_count = 0\n",
    "        for tid in tokens:\n",
    "            if tid != pad_token_id:\n",
    "                if np.random.random() > self.corr_prob:\n",
    "                    mt_encoder_input_tokens.append(tid)\n",
    "                    mt_encoder_output_tokens.append(pad_token_id)\n",
    "                else:\n",
    "                    mt_encoder_input_tokens.append(mask_token_id)\n",
    "                    mt_encoder_output_tokens.append(tid)\n",
    "                    mask_count += 1\n",
    "            else:\n",
    "                mt_encoder_input_tokens.append(tid)\n",
    "                mt_encoder_output_tokens.append(tid)\n",
    "        \n",
    "        if mask_count > 0:\n",
    "            return mt_encoder_input_tokens, mt_encoder_output_tokens\n",
    "        else:\n",
    "            return mt_encoder_input_tokens, -1\n",
    "        \n",
    "    def __create_nt_input_and_output__(self, text):\n",
    "        input_tokens = self.tokenizer(\"[NT]\" + text, add_special_tokens=False, max_length=self.max_len, padding=\"max_length\", \n",
    "                                      truncation=True, return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        output_tokens = self.tokenizer(text + \"[EOS]\", add_special_tokens=False, max_length=self.max_len, padding=\"max_length\", \n",
    "                                      truncation=True, return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        \n",
    "        return input_tokens, output_tokens\n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        df_batch_ms = self.df[batch_slice].copy(deep=True).reset_index(drop=True)\n",
    "        df_batch_mt = df_batch_ms.copy(deep=True)\n",
    "        df_batch_nt = df_batch_ms.copy(deep=True)\n",
    "\n",
    "        df_batch_ms[[\"ms_encoder_input_text\", \"ms_decoder_input_text\", \"ms_decoder_output_text\"]] = df_batch_ms.apply(lambda x: self.__create_ms_input_and_output__(x[\"text\"]), axis=1, result_type=\"expand\")\n",
    "\n",
    "        df_batch_ms = df_batch_ms[df_batch_ms.ms_decoder_output_text != \"\"].reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        df_batch_ms[\"ms_encoder_input_tokens\"] = df_batch_ms[\"ms_encoder_input_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "                                                                                             padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "                                                                                             return_token_type_ids=False)[\"input_ids\"])\n",
    "\n",
    "        df_batch_ms[\"ms_decoder_input_tokens\"] = df_batch_ms[\"ms_decoder_input_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "                                                                                                         padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "                                                                                                         return_token_type_ids=False)[\"input_ids\"])\n",
    "\n",
    "        df_batch_ms[\"ms_decoder_output_tokens\"] = df_batch_ms[\"ms_decoder_output_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "                                                                                                           padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "                                                                                                           return_token_type_ids=False)[\"input_ids\"])\n",
    "        \n",
    "        df_batch_mt[[\"mt_encoder_input_tokens\", \"mt_encoder_output_tokens\"]] = df_batch_mt.apply(lambda x: self.__create_mt_input_and_output__(x[\"text\"]), axis=1, result_type=\"expand\")\n",
    "        df_batch_mt = df_batch_mt[df_batch_mt.mt_encoder_output_tokens != -1]\n",
    "        \n",
    "        df_batch_nt[[\"nt_decoder_input_tokens\", \"nt_decoder_output_tokens\"]] = df_batch_nt.apply(lambda x: self.__create_nt_input_and_output__(x[\"text\"]), axis=1, result_type=\"expand\")\n",
    "        \n",
    "        \n",
    "\n",
    "        ms_encoder_input_array = np.array(df_batch_ms[\"ms_encoder_input_tokens\"].tolist())\n",
    "        ms_decoder_input_array = np.array(df_batch_ms[\"ms_decoder_input_tokens\"].tolist())\n",
    "        ms_decoder_output_array = np.array(df_batch_ms[\"ms_decoder_output_tokens\"].tolist())\n",
    "        \n",
    "        mt_encoder_input_array = np.array(df_batch_mt[\"mt_encoder_input_tokens\"].tolist())\n",
    "        mt_encoder_output_array = np.array(df_batch_mt[\"mt_encoder_output_tokens\"].tolist())\n",
    "        \n",
    "        nt_decoder_input_array = np.array(df_batch_nt[\"nt_decoder_input_tokens\"].tolist())\n",
    "        nt_decoder_output_array = np.array(df_batch_nt[\"nt_decoder_output_tokens\"].tolist())\n",
    "        \n",
    "        return (ms_encoder_input_array, ms_decoder_input_array, mt_encoder_input_array, nt_decoder_input_array), (ms_decoder_output_array, mt_encoder_output_array, nt_decoder_output_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d61b8d2-1b06-4fe3-8dbc-804caaafb8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# schduler\n",
    "class LinearLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, df, batch_size, num_epochs, initial_lr):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.num_steps = tf.cast((df.shape[0]*1.1 // batch_size) * num_epochs, tf.float32)\n",
    "        self.initial_lr = tf.cast(initial_lr, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        \n",
    "    \n",
    "        return self.initial_lr * (self.num_steps - step)/self.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43c95219-53b8-4f39-905b-293fe99de4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and metrics\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "    \n",
    "    mask = label != 0\n",
    "    \n",
    "    match = match & mask\n",
    "    \n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2058c4e3-a0f9-4358-902d-21845ff3f01c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get model\n",
    "def enocoder_layer_consolidated(encoder_input, embedding_layer, pos_embedding_layer, encoder_layer):\n",
    "    embedding_output = embedding_layer(encoder_input, embedding_type=\"input\")\n",
    "    pos_embedding_output = pos_embedding_layer(embedding_output)\n",
    "    encoder_output = encoder_layer(pos_embedding_output)\n",
    "    return encoder_output\n",
    "\n",
    "def decoder_layer_consolidated(encoder_output, decoder_input, embedding_layer, pos_embedding_layer, decoder_layer, gpt_mode=False):\n",
    "    embedding_output = embedding_layer(decoder_input, embedding_type=\"input\")\n",
    "    pos_embedding_output = pos_embedding_layer(embedding_output)\n",
    "    decoder_output = decoder_layer(pos_embedding_output, encoder_output, gpt_mode)\n",
    "    return decoder_output\n",
    "\n",
    "\n",
    "# get model\n",
    "def get_model():\n",
    "    ms_encoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"ms_encoder_input\")\n",
    "    mt_encoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"mt_encoder_input\")\n",
    "\n",
    "    ms_decoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"ms_decoder_input\")\n",
    "    nt_decoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"nt_decoder_input\")\n",
    "    \n",
    "    \n",
    "    embedding_layer = EmbeddingLayer(vocab_size, d_model)\n",
    "    pos_embedding_layer = PositionalEmbedding(d_model, max_len)\n",
    "    encoder_layer = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                             num_heads=num_heads, dff=dff,\n",
    "                             dropout_rate=dropout_rate)\n",
    "    decoder_layer = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                             num_heads=num_heads, dff=dff,\n",
    "                             dropout_rate=dropout_rate)\n",
    "\n",
    "    ms_encoder_output = enocoder_layer_consolidated(ms_encoder_input, embedding_layer, pos_embedding_layer, encoder_layer)\n",
    "    mt_encoder_output = enocoder_layer_consolidated(mt_encoder_input, embedding_layer, pos_embedding_layer, encoder_layer)\n",
    "    \n",
    "    ms_decoder_output = decoder_layer_consolidated(ms_encoder_output, ms_decoder_input, embedding_layer, pos_embedding_layer, decoder_layer)\n",
    "    nt_decoder_output = decoder_layer_consolidated(None, nt_decoder_input, embedding_layer, pos_embedding_layer, decoder_layer, True)\n",
    "\n",
    "    y_ms = embedding_layer(ms_decoder_output, embedding_type=\"output\")\n",
    "    y_ms = tf.keras.layers.Activation(\"linear\", name=\"ms_output\")(y_ms)\n",
    "\n",
    "    y_mt = embedding_layer(mt_encoder_output, embedding_type=\"output\")\n",
    "    y_mt = tf.keras.layers.Activation(\"linear\", name=\"mt_output\")(y_mt)\n",
    "\n",
    "    y_nt = embedding_layer(nt_decoder_output, embedding_type=\"output\")\n",
    "    y_nt = tf.keras.layers.Activation(\"linear\", name=\"nt_output\")(y_nt)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "        # b/250038731\n",
    "        del y_ms._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "        # b/250038731\n",
    "        del y_mt._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "        # b/250038731\n",
    "        del y_nt._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    model = tf.keras.models.Model([ms_encoder_input, ms_decoder_input, mt_encoder_input, nt_decoder_input], [y_ms, y_mt, y_nt])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc6644b4-63fa-448d-be36-1810ed907a86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " ms_decoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " ms_encoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer_2 (EmbeddingLa  multiple            25678728    ['ms_encoder_input[0][0]',       \n",
      " yer)                                                             'mt_encoder_input[0][0]',       \n",
      "                                                                  'ms_decoder_input[0][0]',       \n",
      "                                                                  'nt_decoder_input[0][0]',       \n",
      "                                                                  'decoder_2[0][0]',              \n",
      "                                                                  'encoder_2[1][0]',              \n",
      "                                                                  'decoder_2[1][0]']              \n",
      "                                                                                                  \n",
      " positional_embedding_2 (Positi  (None, 256, 512)    0           ['embedding_layer_2[0][0]',      \n",
      " onalEmbedding)                                                   'embedding_layer_2[1][0]',      \n",
      "                                                                  'embedding_layer_2[2][0]',      \n",
      "                                                                  'embedding_layer_2[3][0]']      \n",
      "                                                                                                  \n",
      " encoder_2 (Encoder)            (None, 256, 512)     18914304    ['positional_embedding_2[0][0]', \n",
      "                                                                  'positional_embedding_2[1][0]'] \n",
      "                                                                                                  \n",
      " decoder_2 (Decoder)            (None, 256, 512)     25224192    ['positional_embedding_2[2][0]', \n",
      "                                                                  'encoder_2[0][0]',              \n",
      "                                                                  'positional_embedding_2[3][0]'] \n",
      "                                                                                                  \n",
      " mt_encoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " nt_decoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " ms_output (Activation)         (None, 256, 50056)   0           ['embedding_layer_2[4][0]']      \n",
      "                                                                                                  \n",
      " mt_output (Activation)         (None, 256, 50056)   0           ['embedding_layer_2[5][0]']      \n",
      "                                                                                                  \n",
      " nt_output (Activation)         (None, 256, 50056)   0           ['embedding_layer_2[6][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 69,817,224\n",
      "Trainable params: 69,817,224\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# compile and fit\n",
    "# Create a MirroredStrategy (uses all available GPUs by default)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# with strategy.scope():\n",
    "#     model = get_model()\n",
    "#     print(model.summary())\n",
    "#     learning_rate = LinearLRSchedule(df, batch_size, num_epochs, 5e-5)\n",
    "#     optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=learning_rate)\n",
    "    \n",
    "#     model.compile(loss=[masked_loss, masked_loss], optimizer=optimizer)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model = get_model()\n",
    "    print(model.summary())\n",
    "    learning_rate = LinearLRSchedule(df, batch_size, num_epochs, 5e-5)\n",
    "    optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss={\"ms_output\": masked_loss, \"mt_output\": masked_loss, \"nt_output\": masked_loss}, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics={\"ms_output\": masked_accuracy, \"mt_output\": masked_accuracy, \"nt_output\": masked_accuracy})\n",
    "\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(SM_MODEL_DIR, \"{epoch}.weights.h5\"), save_weights_only=True)\n",
    "logs = tf.keras.callbacks.CSVLogger(os.path.join(SM_MODEL_DIR, \"logs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7c721-dec1-4179-9d19-11d8b354a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1889/441971 [..............................] - ETA: 278:28:35 - loss: 23.4534 - ms_output_loss: 5.6977 - mt_output_loss: 9.1868 - nt_output_loss: 8.5689 - ms_output_masked_accuracy: 0.3858 - mt_output_masked_accuracy: 0.0136 - nt_output_masked_accuracy: 0.0855"
     ]
    }
   ],
   "source": [
    "# train\n",
    "gen = BatchGenerator(df, batch_size)\n",
    "model.fit(gen, epochs=num_epochs, callbacks=[checkpoint, logs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babb1d0-2408-4050-ac35-784edceec3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
