{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26d477-f1d5-4e68-b45a-e3ebeac755fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a80898a-0c3a-4f21-ba5e-ebbab2a0699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 12:33:12.673327: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-06 12:33:12.789093: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-06 12:33:13.471607: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-06 12:33:13.471690: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-06 12:33:13.471699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 12:33:17.258379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-07-06 12:33:17.260409: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-06 12:33:17.260494: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-06 12:33:17.260552: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-06 12:33:17.260634: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-06 12:33:17.260689: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-06 12:33:17.260743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-06 12:33:17.260755: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import gc\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "# from pandarallel import pandarallel\n",
    "\n",
    "# pandarallel.initialize(nb_workers=12)\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fa6948-0620-43f1-916e-cc8d71811e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# SM_CHANNEL_TRAIN = \"/opt/ml/data/train\"\n",
    "# SM_MODEL_DIR = \"/opt/ml/model\"\n",
    "\n",
    "\n",
    "SM_CHANNEL_TRAIN = \"pretrain_data/\"\n",
    "SM_MODEL_DIR = \"/opt/ml/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c9a8b5c-ea4a-4d92-80e5-57842af6abef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    \n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "    \n",
    "    pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "#     def __init__(self, vocab_size, d_model):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "#         self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "#     def compute_mask(self, *args, **kwargs):\n",
    "#         return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         length = tf.shape(x)[1]\n",
    "#         x = self.embedding(x)\n",
    "#         # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "#         x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "#         return x, self.embedding\n",
    "\n",
    "\n",
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True, name=\"shared_embedding\") \n",
    "        self.output_bias = self.add_weight(shape=(vocab_size,), initializer=\"zeros\", trainable=True, name=\"output_bias\")\n",
    "    \n",
    "    def call(self, x, embedding_type):\n",
    "        if embedding_type == \"input\":\n",
    "            x = self.embedding(x)\n",
    "            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "            return x\n",
    "        elif embedding_type == \"output\":\n",
    "            x = tf.matmul(x, self.embedding.embeddings, transpose_b=True)\n",
    "            return x + self.output_bias\n",
    "        else:\n",
    "            raise ValueError(\"embedding_type must be 'input' or 'output'\")\n",
    "        \n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = positional_encoding(length=max_len, depth=d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75785a98-4508-4b73-97eb-b63cf2df7db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# attention\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "        \n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "        \n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77131ca8-c406-48a2-90a7-6cc2b9fb0adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feedforward\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='gelu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "87676420-2476-412f-9062-34d307300739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model//num_heads,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# class Encoder(tf.keras.layers.Layer):\n",
    "#     def __init__(self, *, num_layers, d_model, num_heads,\n",
    "#                  dff, vocab_size, dropout_rate=0.1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.d_model = d_model\n",
    "#         self.num_layers = num_layers\n",
    "        \n",
    "#         self.pos_embedding = PositionalEmbedding(\n",
    "#             vocab_size=vocab_size, d_model=d_model)\n",
    "        \n",
    "#         self.enc_layers = [\n",
    "#             EncoderLayer(d_model=d_model,\n",
    "#                          num_heads=num_heads,\n",
    "#                          dff=dff,\n",
    "#                          dropout_rate=dropout_rate)\n",
    "#             for _ in range(num_layers)]\n",
    "#         self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # `x` is token-IDs shape: (batch, seq_len)\n",
    "#         x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        \n",
    "#         # Add dropout.\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         for i in range(self.num_layers):\n",
    "#             x = self.enc_layers[i](x)\n",
    "        \n",
    "#         return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "58671078-06de-411e-8447-5600ddefb82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decoder\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "        \n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "        \n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "    \n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context)\n",
    "    \n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "    \n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99535292-ce4d-47e5-9527-6a71ed07603c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(SM_CHANNEL_TRAIN, \"btokenizer\"))\n",
    "\n",
    "new_spl_tokens = [\"[SOS]\", \"[EOS]\", \"[SPANEND]\", \"[MS]\", \"[MT]\"] + [f\"[SPAN{i}]\" for i in range(50)]\n",
    "\n",
    "tokenizer.add_tokens(new_tokens=new_spl_tokens, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3b15c3f-7e7f-42f8-90ce-0d110886e1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# num_layers = 12\n",
    "# d_model = 768\n",
    "# dff = 3072\n",
    "# num_heads = 12\n",
    "# dropout_rate = 0.1\n",
    "# max_len = 256\n",
    "# corr_prob = 0.15\n",
    "# vocab_size = len(tokenizer)\n",
    "\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "max_len = 256\n",
    "corr_prob = 0.15\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd6ccebc-879c-4030-84db-80a8b8487f03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7429"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "\n",
    "with open(os.path.join(SM_CHANNEL_TRAIN, \"raw_text.txt\"), \"r\") as f:\n",
    "    data = f.read().split(\"\\n\")\n",
    "\n",
    "data = list(set(data))\n",
    "\n",
    "df = pd.DataFrame({\"text\": data}).sample(frac=1.0, ignore_index=True)\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d166b306-89fd-480b-b95a-6a0472e79fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "class BatchGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size, tokenizer=tokenizer, max_len=max_len, corr_prob=corr_prob):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.corr_prob = corr_prob\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.df) % self.batch_size == 0:\n",
    "            return len(self.df) // self.batch_size\n",
    "        return len(self.df) // self.batch_size + 1\n",
    "\n",
    "    def __create_ms_input_and_output__(self, text):\n",
    "        corr_count = 0\n",
    "        text_tokens = text.split(\" \")\n",
    "        input_tokens = [\"[MS]\"]\n",
    "        output_tokens = []\n",
    "        prev_token_corr = False\n",
    "        for token in text_tokens:\n",
    "            if np.random.random() > self.corr_prob:\n",
    "                input_tokens.append(token)\n",
    "                prev_token_corr = False\n",
    "            else:\n",
    "                if not prev_token_corr:\n",
    "                    input_tokens.append(f\"[SPAN{corr_count}]\")\n",
    "                    \n",
    "                    output_tokens.append(f\"[SPAN{corr_count}]\")\n",
    "                    corr_count += 1\n",
    "                output_tokens.append(token)\n",
    "                prev_token_corr = True\n",
    "                \n",
    "        output_tokens.append(\"[SPANEND]\")\n",
    "\n",
    "        return \" \".join(input_tokens), \" \".join(output_tokens[:-1]), \" \".join(output_tokens[1:])\n",
    "    \n",
    "    def __create_mt_input_and_output__(self, text):\n",
    "        \n",
    "        tokens = self.tokenizer(text, add_special_tokens=False, max_length=self.max_len-1, padding=\"max_length\", \n",
    "                                truncation=True, return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        pad_token_id = self.tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "        mask_token_id = self.tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "        mt_encoder_input_tokens = [self.tokenizer.convert_tokens_to_ids(\"[MT]\")]\n",
    "        mt_encoder_output_tokens = [pad_token_id]\n",
    "        mask_count = 0\n",
    "        for tid in tokens:\n",
    "            if tid != pad_token_id:\n",
    "                if np.random.random() > self.corr_prob:\n",
    "                    mt_encoder_input_tokens.append(tid)\n",
    "                    mt_encoder_output_tokens.append(pad_token_id)\n",
    "                else:\n",
    "                    mt_encoder_input_tokens.append(mask_token_id)\n",
    "                    mt_encoder_output_tokens.append(tid)\n",
    "                    mask_count += 1\n",
    "            else:\n",
    "                mt_encoder_input_tokens.append(tid)\n",
    "                mt_encoder_output_tokens.append(tid)\n",
    "        \n",
    "        if mask_count > 0:\n",
    "            return mt_encoder_input_tokens, mt_encoder_output_tokens\n",
    "        else:\n",
    "            return mt_encoder_input_tokens, -1\n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        # df_batch_ms = self.df[batch_slice].copy(deep=True).reset_index(drop=True)\n",
    "        df_batch_mt = self.df[batch_slice].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "        # df_batch_ms[[\"ms_encoder_input_text\", \"ms_decoder_input_text\", \"ms_decoder_output_text\"]] = df_batch_ms.apply(lambda x: self.__create_ms_input_and_output__(x[\"text\"]), axis=1, result_type=\"expand\")\n",
    "\n",
    "        # df_batch_ms = df_batch_ms[df_batch_ms.ms_decoder_output_text != \"\"].reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        # df_batch_ms[\"ms_encoder_input_tokens\"] = df_batch_ms[\"ms_encoder_input_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "        #                                                                                      padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "        #                                                                                      return_token_type_ids=False)[\"input_ids\"])\n",
    "\n",
    "        # df_batch_ms[\"ms_decoder_input_tokens\"] = df_batch_ms[\"ms_decoder_input_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "        #                                                                                                  padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "        #                                                                                                  return_token_type_ids=False)[\"input_ids\"])\n",
    "\n",
    "        # df_batch_ms[\"ms_decoder_output_tokens\"] = df_batch_ms[\"ms_decoder_output_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "        #                                                                                                    padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "        #                                                                                                    return_token_type_ids=False)[\"input_ids\"])\n",
    "        \n",
    "        df_batch_mt[[\"mt_encoder_input_tokens\", \"mt_encoder_output_tokens\"]] = df_batch_mt.apply(lambda x: self.__create_mt_input_and_output__(x[\"text\"]), axis=1, result_type=\"expand\")\n",
    "        df_batch_mt = df_batch_mt[df_batch_mt.mt_encoder_output_tokens != -1]\n",
    "        \n",
    "        \n",
    "\n",
    "        # ms_encoder_input_array = np.array(df_batch_ms[\"ms_encoder_input_tokens\"].tolist())\n",
    "        # ms_decoder_input_array = np.array(df_batch_ms[\"ms_decoder_input_tokens\"].tolist())\n",
    "        # ms_decoder_output_array = np.array(df_batch_ms[\"ms_decoder_output_tokens\"].tolist())\n",
    "        \n",
    "        mt_encoder_input_array = np.array(df_batch_mt[\"mt_encoder_input_tokens\"].tolist())\n",
    "        mt_encoder_output_array = np.array(df_batch_mt[\"mt_encoder_output_tokens\"].tolist())\n",
    "        \n",
    "        # return (ms_encoder_input_array, ms_decoder_input_array, mt_encoder_input_array), (ms_decoder_output_array, mt_encoder_output_array)\n",
    "        return mt_encoder_input_array, mt_encoder_output_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d61b8d2-1b06-4fe3-8dbc-804caaafb8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# schduler\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    \n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43c95219-53b8-4f39-905b-293fe99de4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and metrics\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 3\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "    \n",
    "    mask = label != 3\n",
    "    \n",
    "    match = match & mask\n",
    "    \n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2058c4e3-a0f9-4358-902d-21845ff3f01c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " mt_encoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer_25 (EmbeddingL  multiple            25678215    ['mt_encoder_input[0][0]',       \n",
      " ayer)                                                            'encoder_28[0][0]']             \n",
      "                                                                                                  \n",
      " positional_embedding_27 (Posit  (None, 256, 512)    0           ['embedding_layer_25[0][0]']     \n",
      " ionalEmbedding)                                                                                  \n",
      "                                                                                                  \n",
      " encoder_28 (Encoder)           (None, 256, 512)     18914304    ['positional_embedding_27[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 44,592,519\n",
      "Trainable params: 44,592,519\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # get model\n",
    "# def get_model():\n",
    "#     ms_encoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"ms_encoder_input\")\n",
    "#     ms_decoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"ms_decoder_input\")\n",
    "    \n",
    "#     mt_encoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"mt_encoder_input\")\n",
    "    \n",
    "#     encoder_layer = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "#                              num_heads=num_heads, dff=dff,\n",
    "#                              vocab_size=vocab_size,\n",
    "#                              dropout_rate=dropout_rate)\n",
    "    \n",
    "#     decoder_layer = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "#                              num_heads=num_heads, dff=dff,\n",
    "#                              vocab_size=vocab_size,\n",
    "#                              dropout_rate=dropout_rate)\n",
    "    \n",
    "#     ms_encoder_output = encoder_layer(ms_encoder_input)\n",
    "#     mt_encoder_output = encoder_layer(mt_encoder_input)\n",
    "\n",
    "#     ms_decoder_output = decoder_layer(ms_decoder_input, ms_encoder_output)\n",
    "\n",
    "#     y_ms = tf.keras.layers.Dense(vocab_size, name=\"ms_output\")(ms_decoder_output)\n",
    "#     y_mt = tf.keras.layers.Dense(vocab_size, name=\"mt_output\")(mt_encoder_output)\n",
    "\n",
    "#     try:\n",
    "#         # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "#         # b/250038731\n",
    "#         del y_ms._keras_mask\n",
    "#     except AttributeError:\n",
    "#         pass\n",
    "    \n",
    "#     try:\n",
    "#         # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "#         # b/250038731\n",
    "#         del y_mt._keras_mask\n",
    "#     except AttributeError:\n",
    "#         pass\n",
    "\n",
    "#     model = tf.keras.models.Model([ms_encoder_input, ms_decoder_input, mt_encoder_input], [y_ms, y_mt])\n",
    "#     return model\n",
    "\n",
    "# model = get_model()\n",
    "# model.summary()\n",
    "\n",
    "def enocoder_layer_consolidated(encoder_input, embedding_layer, pos_embedding_layer, encoder_layer):\n",
    "    embedding_output = embedding_layer(encoder_input, embedding_type=\"input\")\n",
    "    pos_embedding_output = pos_embedding_layer(embedding_output)\n",
    "    encoder_output = encoder_layer(pos_embedding_output)\n",
    "    return encoder_output\n",
    "\n",
    "\n",
    "# get model\n",
    "def get_model():\n",
    "    mt_encoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"mt_encoder_input\")\n",
    "    \n",
    "    embedding_layer = EmbeddingLayer(vocab_size, d_model)\n",
    "    pos_embedding_layer = PositionalEmbedding(d_model, max_len)\n",
    "    encoder_layer = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                             num_heads=num_heads, dff=dff,\n",
    "                             dropout_rate=dropout_rate)\n",
    "\n",
    "    mt_encoder_output = enocoder_layer_consolidated(mt_encoder_input, embedding_layer, pos_embedding_layer, encoder_layer)\n",
    "    y_mt = embedding_layer(mt_encoder_output, embedding_type=\"output\")\n",
    "    \n",
    "    try:\n",
    "        # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "        # b/250038731\n",
    "        del y_mt._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    model = tf.keras.models.Model(mt_encoder_input, y_mt)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fc6644b4-63fa-448d-be36-1810ed907a86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1206/441971 [..............................] - ETA: 108:09:24 - loss: 9.6679 - masked_accuracy: 0.0115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7ff3da3eb7f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/weakref.py\", line 370, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mCSVLogger(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SM_MODEL_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     15\u001b[0m gen \u001b[38;5;241m=\u001b[39m BatchGenerator(df, \u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compile and fit\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=masked_accuracy)\n",
    "\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(SM_MODEL_DIR, \"{epoch}.weights.h5\"), save_weights_only=True)\n",
    "logs = tf.keras.callbacks.CSVLogger(os.path.join(SM_MODEL_DIR, \"logs.csv\"))\n",
    "\n",
    "\n",
    "gen = BatchGenerator(df, 16)\n",
    "model.fit(gen, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8d624-8e5d-46c5-84ab-f6047d734752",
   "metadata": {},
   "source": [
    "# Train Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac6b437-9510-4e31-9436-9019fd4859f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cae7102-68ca-4264-b989-cd7155ca1067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_spl_tokens = [\"[SOS]\", \"[EOS]\", \"[SPANEND]\"] + [f\"[SPAN{i}]\" for i in range(50)]\n",
    "\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=50000, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5001efff-f964-4a2d-99d8-8dbd7b499489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files=[\"pretrain_data/raw_text.txt\"], trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7715212b-3631-4287-9f3a-8fa2c0cfdf7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save(\"pretrain_data/btokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a8296e2-d90e-4204-b8f7-988069d1971b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"pretrain_data/btokenizer.json\", cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    mask_token=\"[MASK]\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba42307e-27b8-4d04-a0a4-49ca238be548",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1831, 1929, 4131, 4324, 3842, 2132, 7152, 14845, 1880, 32805]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(df.text[0], max_length=10, padding=\"max_length\", truncation=True, add_special_tokens=False, return_attention_mask=False, \n",
    "                                                                                             return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d819b2df-9c41-4186-9108-2d0a534f6f04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokenize(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25e5d3-09aa-4739-a7f1-eb5fcff667af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0032927-2654-4272-9bdc-ca7c4a1046d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pretrain_data/btokenizer/tokenizer_config.json',\n",
       " 'pretrain_data/btokenizer/special_tokens_map.json',\n",
       " 'pretrain_data/btokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.save_pretrained(\"pretrain_data/btokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323cead2-8099-4e4c-a9e8-4211d2cedf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.convert_ids_to_tokens(16216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a10bd437-9e77-4d7a-a5fa-624f320cc7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772ff63e-f916-466e-828f-597fc1fcf34a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained(\"pretrain_data/btokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c031f89-ecf3-46d2-aea2-263d337f0c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(df\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(df.text[0]).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79fa833e-0378-430c-9a90-63cf77219453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cac3ace9-df26-42d8-b041-5cc8ddd598d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7c721-dec1-4179-9d19-11d8b354a22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.p3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
