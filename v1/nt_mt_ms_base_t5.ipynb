{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26d477-f1d5-4e68-b45a-e3ebeac755fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a80898a-0c3a-4f21-ba5e-ebbab2a0699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 21:01:53.767921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-30 21:01:55.071321: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/site-packages/smdistributed/dataparallel/lib:/usr/local/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib\n",
      "2025-06-30 21:01:55.071449: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/site-packages/smdistributed/dataparallel/lib:/usr/local/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib\n",
      "2025-06-30 21:01:55.071464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import gc\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "# from pandarallel import pandarallel\n",
    "\n",
    "# pandarallel.initialize(nb_workers=12)\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fa6948-0620-43f1-916e-cc8d71811e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# SM_CHANNEL_TRAIN = \"/opt/ml/input/data/train\"\n",
    "# SM_MODEL_DIR = \"/opt/ml/model\"\n",
    "\n",
    "\n",
    "SM_CHANNEL_TRAIN = \"pretrain_data/\"\n",
    "SM_MODEL_DIR = \"/opt/ml/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9a8b5c-ea4a-4d92-80e5-57842af6abef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    \n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "    \n",
    "    pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75785a98-4508-4b73-97eb-b63cf2df7db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# attention\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "        \n",
    "        # Cache the attention scores for plotting later.\n",
    "        # self.last_attn_scores = attn_scores\n",
    "        \n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77131ca8-c406-48a2-90a7-6cc2b9fb0adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feedforward\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87676420-2476-412f-9062-34d307300739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "        \n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        \n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58671078-06de-411e-8447-5600ddefb82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decoder\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, gpt_mode):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        \n",
    "        if not gpt_mode:\n",
    "            x = self.cross_attention(x=x, context=context)\n",
    "        \n",
    "        # Cache the last attention scores for plotting later\n",
    "        # self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "        \n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "    \n",
    "        # self.last_attn_scores = None\n",
    "        \n",
    "\n",
    "    def call(self, x, context, gpt_mode=False):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if gpt_mode:\n",
    "            context = self.pos_embedding(context)  # (batch_size, target_seq_len, d_model)\n",
    "            context = self.dropout(context)\n",
    "        \n",
    "        # context = self.update_context(context)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context, gpt_mode)\n",
    "    \n",
    "        # self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "    \n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99535292-ce4d-47e5-9527-6a71ed07603c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(SM_CHANNEL_TRAIN, \"btokenizer\"))\n",
    "\n",
    "new_spl_tokens = [\"[SOS]\", \"[EOS]\", \"[SPANEND]\", \"[MS]\", \"[MT]\", \"[NT]\"] + [f\"[SPAN{i}]\" for i in range(50)]\n",
    "\n",
    "tokenizer.add_tokens(new_tokens=new_spl_tokens, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b15c3f-7e7f-42f8-90ce-0d110886e1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# num_layers = 12\n",
    "# d_model = 768\n",
    "# dff = 3072\n",
    "# num_heads = 12\n",
    "# dropout_rate = 0.1\n",
    "# max_len = 256\n",
    "# corr_prob = 0.15\n",
    "# vocab_size = len(tokenizer)\n",
    "\n",
    "num_layers = 2\n",
    "d_model = 128\n",
    "dff = 1024\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1\n",
    "max_len = 256\n",
    "corr_prob = 0.15\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd6ccebc-879c-4030-84db-80a8b8487f03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "\n",
    "with open(os.path.join(SM_CHANNEL_TRAIN, \"raw_text.txt\"), \"r\") as f:\n",
    "    data = f.read().split(\"\\n\")\n",
    "\n",
    "data = list(set(data))\n",
    "\n",
    "df = pd.DataFrame({\"text\": data}).sample(frac=1.0, ignore_index=True)\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d166b306-89fd-480b-b95a-6a0472e79fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "class BatchGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size, tokenizer=tokenizer, max_len=max_len, corr_prob=corr_prob):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.corr_prob = corr_prob\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.df) % self.batch_size == 0:\n",
    "            return len(self.df) // self.batch_size\n",
    "        return len(self.df) // self.batch_size + 1\n",
    "\n",
    "    def __create_ms_input_and_output__(self, text):\n",
    "        corr_count = 0\n",
    "        text_tokens = text.split(\" \")\n",
    "        input_tokens = [\"[MS]\"]\n",
    "        output_tokens = []\n",
    "        prev_token_corr = False\n",
    "        for token in text_tokens:\n",
    "            if np.random.random() > self.corr_prob:\n",
    "                input_tokens.append(token)\n",
    "                prev_token_corr = False\n",
    "            else:\n",
    "                if not prev_token_corr:\n",
    "                    input_tokens.append(f\"[SPAN{corr_count}]\")\n",
    "                    \n",
    "                    output_tokens.append(f\"[SPAN{corr_count}]\")\n",
    "                    corr_count += 1\n",
    "                output_tokens.append(token)\n",
    "                prev_token_corr = True\n",
    "                \n",
    "        output_tokens.append(\"[SPANEND]\")\n",
    "\n",
    "        return \" \".join(input_tokens), \" \".join(output_tokens[:-1]), \" \".join(output_tokens[1:])\n",
    "    \n",
    "    def __create_mt_input_and_output__(self, text):\n",
    "        \n",
    "        tokens = self.tokenizer(text, add_special_tokens=False, max_length=self.max_len, padding=\"max_length\", \n",
    "                                truncation=True, return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        pad_token_id = self.tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "        mask_token_id = self.tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "        mt_encoder_input_tokens = [self.tokenizer.convert_tokens_to_ids(\"[MT]\")]\n",
    "        mt_encoder_output_tokens = [pad_token_id]\n",
    "        mask_count = 0\n",
    "        for tid in tokens:\n",
    "            if tid != pad_token_id:\n",
    "                if np.random.random() > self.corr_prob:\n",
    "                    mt_encoder_input_tokens.append(tid)\n",
    "                    mt_encoder_output_tokens.append(pad_token_id)\n",
    "                else:\n",
    "                    mt_encoder_input_tokens.append(mask_token_id)\n",
    "                    mt_encoder_output_tokens.append(tid)\n",
    "                    mask_count += 1\n",
    "            else:\n",
    "                mt_encoder_input_tokens.append(tid)\n",
    "                mt_encoder_output_tokens.append(tid)\n",
    "        \n",
    "        if mask_count > 0:\n",
    "            return mt_encoder_input_tokens, mt_encoder_output_tokens\n",
    "        else:\n",
    "            return mt_encoder_input_tokens, -1\n",
    "        \n",
    "    def __create_nt_input_and_output__(self, text):\n",
    "        input_tokens = self.tokenizer(\"[NT]\" + text, add_special_tokens=False, max_length=self.max_len, padding=\"max_length\", \n",
    "                                      truncation=True, return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        output_tokens = self.tokenizer(text + \"[EOS]\", add_special_tokens=False, max_length=self.max_len, padding=\"max_length\", \n",
    "                                      truncation=True, return_attention_mask=False, return_token_type_ids=False)[\"input_ids\"]\n",
    "        \n",
    "        return input_tokens, output_tokens\n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_slice = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        df_batch_ms = self.df[batch_slice].copy(deep=True).reset_index(drop=True)\n",
    "        df_batch_mt = df_batch_ms.copy(deep=True)\n",
    "        df_batch_nt = df_batch_ms.copy(deep=True)\n",
    "\n",
    "        df_batch_ms[[\"ms_encoder_input_text\", \"ms_decoder_input_text\", \"ms_decoder_output_text\"]] = df_batch_ms.apply(lambda x: self.__create_ms_input_and_output__(x[\"text\"]), axis=1, result_type=\"expand\")\n",
    "\n",
    "        df_batch_ms = df_batch_ms[df_batch_ms.ms_decoder_output_text != \"\"].reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        df_batch_ms[\"ms_encoder_input_tokens\"] = df_batch_ms[\"ms_encoder_input_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "                                                                                             padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "                                                                                             return_token_type_ids=False)[\"input_ids\"])\n",
    "\n",
    "        df_batch_ms[\"ms_decoder_input_tokens\"] = df_batch_ms[\"ms_decoder_input_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "                                                                                                         padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "                                                                                                         return_token_type_ids=False)[\"input_ids\"])\n",
    "\n",
    "        df_batch_ms[\"ms_decoder_output_tokens\"] = df_batch_ms[\"ms_decoder_output_text\"].apply(lambda x: self.tokenizer(x, add_special_tokens=False, max_length=self.max_len, \n",
    "                                                                                                           padding=\"max_length\", truncation=True, return_attention_mask=False, \n",
    "                                                                                                           return_token_type_ids=False)[\"input_ids\"])\n",
    "        \n",
    "        df_batch_mt[[\"mt_encoder_input_tokens\", \"mt_encoder_output_tokens\"]] = df_batch_mt.apply(lambda x: self.__create_mt_input_and_output__(x[\"text\"]), axis=1, result_type=\"expand\")\n",
    "        df_batch_mt = df_batch_mt[df_batch_mt.mt_encoder_output_tokens != -1]\n",
    "        \n",
    "        df_batch_nt[[\"nt_decoder_input_tokens\", \"nt_decoder_output_tokens\"]] = df_batch_nt.apply(lambda x: self.__create_nt_input_and_output__(x[\"text\"]), axis=1, result_type=\"expand\")\n",
    "        \n",
    "        \n",
    "\n",
    "        ms_encoder_input_array = np.array(df_batch_ms[\"ms_encoder_input_tokens\"].tolist())\n",
    "        ms_decoder_input_array = np.array(df_batch_ms[\"ms_decoder_input_tokens\"].tolist())\n",
    "        ms_decoder_output_array = np.array(df_batch_ms[\"ms_decoder_output_tokens\"].tolist())\n",
    "        \n",
    "        mt_encoder_input_array = np.array(df_batch_mt[\"mt_encoder_input_tokens\"].tolist())\n",
    "        mt_encoder_output_array = np.array(df_batch_mt[\"mt_encoder_output_tokens\"].tolist())\n",
    "        \n",
    "        nt_decoder_input_array = np.array(df_batch_nt[\"nt_decoder_input_tokens\"].tolist())\n",
    "        nt_decoder_output_array = np.array(df_batch_nt[\"nt_decoder_output_tokens\"].tolist())\n",
    "        \n",
    "        return (ms_encoder_input_array, ms_decoder_input_array, mt_encoder_input_array, nt_decoder_input_array), (ms_decoder_output_array, mt_encoder_output_array, nt_decoder_output_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d61b8d2-1b06-4fe3-8dbc-804caaafb8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# schduler\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    \n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43c95219-53b8-4f39-905b-293fe99de4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and metrics\n",
    "def masked_loss(label, pred):\n",
    "    mask = label != 3\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "    \n",
    "    mask = label != 3\n",
    "    \n",
    "    match = match & mask\n",
    "    \n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2058c4e3-a0f9-4358-902d-21845ff3f01c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " ms_encoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " ms_decoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " encoder_1 (Encoder)            (None, 256, 128)     7198720     ['ms_encoder_input[0][0]',       \n",
      "                                                                  'mt_encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " mt_encoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " nt_decoder_input (InputLayer)  [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " decoder_1 (Decoder)            (None, 256, 128)     7463168     ['ms_decoder_input[0][0]',       \n",
      "                                                                  'encoder_1[0][0]',              \n",
      "                                                                  'nt_decoder_input[0][0]',       \n",
      "                                                                  'nt_decoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " ms_output (Dense)              (None, 256, 50056)   6457224     ['decoder_1[0][0]']              \n",
      "                                                                                                  \n",
      " mt_output (Dense)              (None, 256, 50056)   6457224     ['encoder_1[1][0]']              \n",
      "                                                                                                  \n",
      " nt_output (Dense)              (None, 256, 50056)   6457224     ['decoder_1[1][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,033,560\n",
      "Trainable params: 34,033,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get model\n",
    "def get_model():\n",
    "    ms_encoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"ms_encoder_input\")\n",
    "    ms_decoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"ms_decoder_input\")\n",
    "    \n",
    "    mt_encoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"mt_encoder_input\")\n",
    "    \n",
    "    nt_decoder_input = tf.keras.layers.Input(shape=(max_len, ), name=\"nt_decoder_input\")\n",
    "    \n",
    "    \n",
    "    encoder_layer = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                             num_heads=num_heads, dff=dff,\n",
    "                             vocab_size=vocab_size,\n",
    "                             dropout_rate=dropout_rate)\n",
    "    \n",
    "    decoder_layer = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                             num_heads=num_heads, dff=dff,\n",
    "                             vocab_size=vocab_size,\n",
    "                             dropout_rate=dropout_rate)\n",
    "    \n",
    "    ms_encoder_output = encoder_layer(ms_encoder_input)\n",
    "    mt_encoder_output = encoder_layer(mt_encoder_input)\n",
    "\n",
    "    ms_decoder_output = decoder_layer(ms_decoder_input, ms_encoder_output, False)\n",
    "    nt_decoder_output = decoder_layer(nt_decoder_input, nt_decoder_input, True)\n",
    "\n",
    "    y_ms = tf.keras.layers.Dense(vocab_size, name=\"ms_output\")(ms_decoder_output)\n",
    "    y_mt = tf.keras.layers.Dense(vocab_size, name=\"mt_output\")(mt_encoder_output)\n",
    "    y_nt = tf.keras.layers.Dense(vocab_size, name=\"nt_output\")(nt_decoder_output)\n",
    "\n",
    "    try:\n",
    "        # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "        # b/250038731\n",
    "        del y_ms._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "        # b/250038731\n",
    "        del y_mt._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "        # b/250038731\n",
    "        del y_nt._keras_mask\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    model = tf.keras.models.Model([ms_encoder_input, ms_decoder_input, mt_encoder_input, nt_decoder_input], [y_ms, y_mt, y_nt])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6644b4-63fa-448d-be36-1810ed907a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile and fit\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.experimental.Adam(learning_rate)\n",
    "model.compile(\n",
    "    loss={\"ms_output\": masked_loss, \"mt_output\": masked_loss, \"nt_output\": masked_loss},\n",
    "    optimizer=optimizer,\n",
    "    metrics={\"ms_output\": masked_accuracy, \"mt_output\": masked_accuracy, \"nt_output\": masked_accuracy})\n",
    "\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(SM_MODEL_DIR, \"{epoch}.weights.h5\"), save_weights_only=True)\n",
    "logs = tf.keras.callbacks.CSVLogger(os.path.join(SM_MODEL_DIR, \"logs.csv\"))\n",
    "\n",
    "\n",
    "gen = BatchGenerator(df, 64)\n",
    "model.fit(gen, epochs=10, callbacks=[checkpoint, logs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7c721-dec1-4179-9d19-11d8b354a22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.p3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.11.0 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.11.0-gpu-py39-cu112-ubuntu20.04-sagemaker-v1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
